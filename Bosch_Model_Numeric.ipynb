{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Deep Neural network model to predict production line failures.\n",
    "    \n",
    "<h4> Dataset: Kaggle Bosch Data (over 1.2 Million rows, 900+ columns for numeric data alone)\n",
    "    Huge class imbalance - defects are only 0.6% of the total data.\n",
    "    \n",
    "<h4> Pipeline: \n",
    "    \n",
    "<h4>           1)  Use tf.datasets API to read in sharded datasets containing upsampled numeric columns without loading entire dataset into memory\n",
    " \n",
    "<h4> 2)  Use tf.DNNClassifier to model the data using a deep neural network and serve the data by defining an input_fn()\n",
    "    \n",
    " <h4>    3) Test the data on shareded versions of the test set which are NOT upsampled.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Pre-processing functions for the numerical and categorical files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_dataset(dataset, threshold):\n",
    "    cols = dataset.columns\n",
    "    size = dataset.shape\n",
    "    reduced_dataset = pd.DataFrame()\n",
    "    for i, col in enumerate(cols):\n",
    "        if dataset[cols[i]].count()/size[0] < threshold:\n",
    "            pass\n",
    "        else:\n",
    "            reduced_dataset[col] = dataset[cols[i]]\n",
    "    return reduced_dataset  \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def preprocess_cat(dataset):\n",
    "    ''' As a preprocessing step, remove all columns that are entirely NaNs.\n",
    "    Convert categorical columns to numeric with a Label Binarizer '''\n",
    "    dataset = dataset.fillna(0)\n",
    "    cols = dataset.columns\n",
    "    size = dataset.shape\n",
    "    print(size)\n",
    "    reduced_dataset = pd.DataFrame()\n",
    "    for i, col in enumerate(cols):\n",
    "        le = LabelEncoder()\n",
    "        if len(set(dataset[col].values)) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            to_string = [str(x) for x in dataset[col].values]\n",
    "            reduced_dataset[col] = le.fit_transform(to_string)\n",
    "    print(reduced_dataset.shape)  \n",
    "    return reduced_dataset   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> To make the code performant, instead of using pandas to read in the sharded files, which definitely won't work on large datasets, I use the Tensorflow Dataset API. The model I am building only uses the numerical data, but uses ALL of it. No upsampling of features is done, and I don't drop ANY columns. \n",
    "    \n",
    "<h5> Because there are So many Columns, we saved the column names as a text file that we load using json. The columns used in the Deep Model are not sparse (atleast 50% of data is not NaNs) while the columns used in the Wide Model are extremely Sparse. Later on, we can add categorical features and improve the model, and also perform some feature generation using feature Crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('col_names.txt', 'r') as f:\n",
    "    tot_cols = json.load(f)\n",
    "    \n",
    "with open('col_names_deep.txt', 'r') as f:\n",
    "    deep_cols = json.load(f)\n",
    "\n",
    "wide_cols = [i for i in tot_cols if i not in deep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the label is not in the feature columns\n",
    "print(\"STOP AND FIX IF LABEL IS IN FEATURE COLUMNS \\n\", 'IS LABEL IN FEATURE COLUMNS? \\n',\n",
    "     'Response' in tot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"For Regular DNN Model Use {} Columns\".format(len(tot_cols)))\n",
    "print(\"For wide and deep model Use {} Wide and {} Deep Columns\".format(len(wide_cols), len(deep_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the label columns and set defaults to 0 for all of them.\n",
    "CSV_COLUMNS = tot_cols + ['Response']\n",
    "#COLS_TO_DROP = ['Id']\n",
    "LABEL = 'Response'\n",
    "#[tf.constant([0], dtype = tf.int64)]+ \n",
    "DEFAULTS = [tf.constant([0.0], dtype=tf.float64) for m in range(len(CSV_COLUMNS)-1)] + [tf.constant([0], dtype = tf.int64)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DEFAULTS)\n",
    "# Check if Glob works --- Caution: don't do this with large files on Pandas, but it is okay with TextLineDataset\n",
    "#tf.gfile.Glob('./FInal_Numeric_Datsets/numeric_data_50k_*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH  = os.getcwd() + '/FInal_Numeric_Datsets/numeric_data_50k_0.csv'\n",
    "#shuffled_train_numeric_train.csv'\n",
    "\n",
    "TEST_PATH = os.getcwd() + '/FInal_Numeric_Datsets/test_numeric_data_50k_0.csv'\n",
    "#shuffled_train_numeric_test.csv'\n",
    "\n",
    "#def weight_col(label):\n",
    "#    ''' for a highly imbalanced dataset, assign small weights to \n",
    "#    dominant column and high weights to positive/defective class'''\n",
    "#    return np.array([1 if l == 1 else 1/len(l) for l in label])\n",
    "\n",
    "\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL)\n",
    "            #for c in COLS_TO_DROP:\n",
    "            #    features.pop(c) # drop the index and the Id columns\n",
    "            return features, label\n",
    "\n",
    "    # Create list of files that match pattern\n",
    "        file_list = tf.gfile.Glob(filename)\n",
    "\n",
    "    # Create dataset from file list\n",
    "        dataset = tf.data.TextLineDataset(file_list).skip(1)\n",
    "        dataset = dataset.map(decode_csv)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10*batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "    \n",
    "\n",
    "def get_train():\n",
    "    return read_dataset(PATH, mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid():\n",
    "    return read_dataset(TEST_PATH, mode = tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "#def get_test():\n",
    "#  return read_dataset('PUT PATH HERE', mode = tf.estimator.ModeKeys.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Let's test the _input_fn below on a tiny subset of the data. First train the model on this subset data, and once you are happy with the model, scale it to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    columns = tf.decode_csv(line, record_defaults = DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "    #print(features)\n",
    "    label = features.pop(LABEL)\n",
    "    #for c in COLS_TO_DROP:\n",
    "    #    features.pop(c)\n",
    "    print(\"The Feature set is {}\".format(len(features)))\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = pd.read_csv(os.getcwd() + '/FInal_Numeric_Datsets/numeric_data_50k_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parser_f() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-0d30aaf273a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_line = pd.read_csv(os.getcwd() + '/FInal_Numeric_Datsets/test_numeric_data_50k_0.csv', index = True\n\u001b[0m\u001b[1;32m      2\u001b[0m                        )\n",
      "\u001b[0;31mTypeError\u001b[0m: parser_f() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "test_line = pd.read_csv(os.getcwd() + '/FInal_Numeric_Datsets/test_numeric_data_50k_0.csv', index = True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(line['0'])\n",
    "plt.title('Training Categories')\n",
    "plt.subplot(1,2,2)\n",
    "sns.countplot(test_line['0'])\n",
    "plt.title('Test Categories')\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=1.5, hspace=0.25,\n",
    "                    wspace=0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Feature set is 968\n"
     ]
    }
   ],
   "source": [
    "filenames = tf.gfile.Glob(os.getcwd() + '/FInal_Numeric_Datsets/numeric_data_50k_*.csv')\n",
    "test = tf.data.TextLineDataset(filenames).skip(1)\n",
    "test.output_classes\n",
    "check = test.map(parse_line)\n",
    "#check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMNS = []\n",
    "for col in tot_cols:\n",
    "    INPUT_COLUMNS.append(tf.feature_column.numeric_column(col))\n",
    "    \n",
    "def make_new_features(feats):\n",
    "    ### Add new features if needed -- this makes more sense for the temporal columns \n",
    "    ### which I don't include here or if the features actually meant something physically. \n",
    "    ### Forget for now\n",
    "    return feats\n",
    "\n",
    "\n",
    "WIDE_COLUMNS = []\n",
    "for col in wide_cols:\n",
    "    WIDE_COLUMNS.append(tf.feature_column.numeric_column(col))\n",
    "    \n",
    "\n",
    "DEEP_COLUMNS = []\n",
    "for col in deep_cols:\n",
    "    DEEP_COLUMNS.append(tf.feature_column.numeric_column(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(968, 968)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(INPUT_COLUMNS), len(WIDE_COLUMNS) + len(DEEP_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Models: Define a DNN Classifier on Numerical inputs and experiment with a Wide and Deep Neural Network for Categorical and Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "if os.path.exists(\"./tmp/DNN_model\"):\n",
    "    shutil.rmtree(\"./tmp/DNN_model\")\n",
    "    model_dir = \"./tmp/DNN_model\"\n",
    "else:\n",
    "    model_dir = \"./tmp/DNN_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./tmp/WDNN_model\"):\n",
    "    shutil.rmtree(\"./tmp/WDNN_model\")\n",
    "    model_wdnn_dir = \"./tmp/WDNN_model\"\n",
    "else:\n",
    "    model_wdnn_dir = \"./tmp/WDNN_model\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> An issue with using Premade APIs is revealed below: the dataset is highly imbalanced. As you will see, the model \n",
    "    learns to put everything in the wrong category as that is the way to get the highest accuracy. I need to use either Custom Model to get the model to learn Recall or change the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(model_dir, hidden_units = [128, 64, 32], dropout = 0.1, WDNN = False):\n",
    "    ''' Builds a Regular DNN classifier or a Wide Deep Neural Network'''\n",
    "    if not WDNN:\n",
    "        classifier = tf.estimator.DNNClassifier(\n",
    "        model_dir = model_dir,\n",
    "        feature_columns=INPUT_COLUMNS,\n",
    "        hidden_units=hidden_units,\n",
    "        optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "        n_classes=2,\n",
    "        loss_reduction=tf.losses.Reduction.MEAN,\n",
    "        dropout=0.5,\n",
    "        config=tf.estimator.RunConfig(tf_random_seed = 42))\n",
    "        \n",
    "    else:\n",
    "        classifier = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_wdnn_dir,\n",
    "        linear_feature_columns=DEEP_COLUMNS,\n",
    "        dnn_feature_columns=WIDE_COLUMNS,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        n_classes=2,\n",
    "        loss_reduction=tf.losses.Reduction.MEAN,\n",
    "        config=tf.estimator.RunConfig(tf_random_seed = 42))\n",
    "\n",
    "    return classifier    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './tmp/DNN_model', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x14cd9d940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = models(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./tmp/DNN_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.690138, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.96803\n",
      "INFO:tensorflow:loss = 0.6468393, step = 101 (103.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.04663\n",
      "INFO:tensorflow:loss = 0.6135131, step = 201 (95.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.0435\n",
      "INFO:tensorflow:loss = 0.59597564, step = 301 (95.830 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.03123\n",
      "INFO:tensorflow:loss = 0.60129476, step = 401 (96.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.0407\n",
      "INFO:tensorflow:loss = 0.5544534, step = 501 (96.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.05035\n",
      "INFO:tensorflow:loss = 0.52832013, step = 601 (95.208 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 608 into ./tmp/DNN_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.05993\n",
      "INFO:tensorflow:loss = 0.53537035, step = 701 (94.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.10523\n",
      "INFO:tensorflow:loss = 0.45000023, step = 801 (90.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.12157\n",
      "INFO:tensorflow:loss = 0.39012682, step = 901 (89.160 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./tmp/DNN_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.37049437.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.train(input_fn = get_train(), max_steps = 1000)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.29587483406067"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end-start)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Do some monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Test the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, roc_curve, recall_score, precision_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(prediction, LABEL):\n",
    "    arr = {\"accuracy\": accuracy_score(LABEL, prediction),\n",
    "           \"Confusion_Matrix\": confusion_matrix(LABEL, prediction),\n",
    "           \"F1 score\": f1_score(LABEL,prediction),\n",
    "           \"Recall Score\": recall_score(LABEL, prediction),\n",
    "           \"Precision Score\": precision_score(LABEL, prediction)\n",
    "           }\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, name, input_fn):\n",
    "    y = model.predict(input_fn = input_fn)\n",
    "    predictions = list(y)\n",
    "    pred1=pd.DataFrame(data=predictions)\n",
    "    prediction=pd.DataFrame(data=pred1['class_ids'])\n",
    "    pred=[]\n",
    "    for row in prediction[\"class_ids\"]:\n",
    "        pred.append(row[0])\n",
    "    return pd.DataFrame(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/DNN_model/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = get_predictions(model, 'Validation', get_valid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data in as a pandas dataframe and test the model.\n",
    "labels = test_line['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9512790255805116, 'Confusion_Matrix': array([[47333,  2399],\n",
       "        [   37,   230]]), 'F1 score': 0.15883977900552487, 'Recall Score': 0.8614232209737828, 'Precision Score': 0.08748573602130087}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = calculate(predicted_labels, labels)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Wide and Deep Neural Network for a Max Steps = 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './tmp/WDNN_model', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x18aac0400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./tmp/WDNN_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.69194645, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.937446\n",
      "INFO:tensorflow:loss = 0.67123514, step = 101 (106.674 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.9994\n",
      "INFO:tensorflow:loss = 0.66992784, step = 201 (100.061 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.04529\n",
      "INFO:tensorflow:loss = 0.6510392, step = 301 (95.667 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.03432\n",
      "INFO:tensorflow:loss = 0.64476794, step = 401 (96.682 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.02339\n",
      "INFO:tensorflow:loss = 0.64988077, step = 501 (97.714 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 598 into ./tmp/WDNN_model/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.03031\n",
      "INFO:tensorflow:loss = 0.64682573, step = 601 (97.058 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.02619\n",
      "INFO:tensorflow:loss = 0.64557755, step = 701 (97.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.04704\n",
      "INFO:tensorflow:loss = 0.62275517, step = 801 (95.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.008\n",
      "INFO:tensorflow:loss = 0.6285559, step = 901 (99.207 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./tmp/WDNN_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.61650145.\n"
     ]
    }
   ],
   "source": [
    "model_wdnn = models(model_wdnn_dir, WDNN = True)\n",
    "start = time.time()\n",
    "model_wdnn.train(input_fn = get_train(), max_steps = 1000)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./tmp/WDNN_model/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = get_predictions(model_wdnn, 'Validation', get_valid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.994179883597672, 'Confusion_Matrix': array([[49708,    24],\n",
       "        [  267,     0]]), 'F1 score': 0.0, 'Recall Score': 0.0, 'Precision Score': 0.0}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = calculate(predicted_labels, labels)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
