{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> In this notebook I write a simple Pandas loop to shard the very large numeric and categorical training data into smaller files. \n",
    "    \n",
    "    \n",
    "<h5> This is not super scalable to really large datasets or must just be run overnight. There may be faster ways using Spark. Upsample the training data, so the imbalanced class and balanced classes are matched. Then I can run a regular DNNClassifier without writing a custom model. \n",
    "    \n",
    "<h5> I write code to shard the total numeric and test data separately, and concat the two into a single file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample, shuffle\n",
    "\n",
    "data_path = os.getcwd() + '/FInal_Numeric_Datsets/shuffled_train_numeric_train.csv'\n",
    "\n",
    "def get_sharded_datasets(full_data_path):\n",
    "    ''' Takes data from your data path and shards the numerical data into files after upsampling the defective class\n",
    "    to 25000 for training. Only use this for training'''\n",
    "    chunksize = 50000\n",
    "    for i, chunk in enumerate(pd.read_csv(full_data_path, chunksize=chunksize)):\n",
    "            print(\"Chunk Number: {}\".format(i))\n",
    "            chunk = chunk.drop(columns = ['Id'])\n",
    "            temp_df = chunk[chunk['Response'] == 1]\n",
    "            print(len(temp_df))\n",
    "            pos_df = resample(temp_df, n_samples=25000,random_state=42,replace=True)\n",
    "            full_df = shuffle(pd.concat([chunk, pos_df]), random_state = 42)\n",
    "            print(len(full_df))\n",
    "            full_df.to_csv(os.path.dirname(full_data_path) + \"/numeric_data_50k_{}.csv\".format(i),\n",
    "                        index=False, header = False) # make sure additional index doens't get added and drop Id column\n",
    "    return print(\"Done\")  \n",
    "\n",
    "data_path_cat = os.getcwd() + '/FInal_Numeric_Datsets/shuffled_train_categorical_train.csv'\n",
    "def get_sharded_categorical_datasets(full_data_path):\n",
    "    ''' Takes the path to your categorical data and shards it into files for training'''\n",
    "    chunksize = 50000\n",
    "    for i, chunk in enumerate(pd.read_csv(full_data_path, chunksize=chunksize)):\n",
    "        print(\"Chunk Number: {}\".format(i))\n",
    "        chunk = chunk.drop(columns = ['Id'])\n",
    "        chunk.to_csv(os.path.dirname(full_data_path) + \"/categorical_data_50k_{}.csv\".format(i)\n",
    "                         ,index = False, header = None)\n",
    "    return print(\"Done\") \n",
    "\n",
    "data_path_test = os.getcwd() + '/FInal_Numeric_Datsets/shuffled_train_numeric_test.csv'\n",
    "def get_sharded_test_sets(full_data_path):\n",
    "    ''' Generates a sharded test set from the numerical data only'''\n",
    "    chunksize = 50000\n",
    "    for i, chunk in enumerate(pd.read_csv(full_data_path, chunksize=chunksize)):\n",
    "            print(\"Chunk Number: {}\".format(i))\n",
    "            chunk = chunk.drop(columns = ['Id'])\n",
    "            chunk.to_csv(os.path.dirname(full_data_path) + \"/test_numeric_data_50k_{}.csv\".format(i),\n",
    "                        index=False, header = False) # make sure additional index doens't get added and drop Id column\n",
    "    return print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sharded_datasets(data_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sharded_categorical_datasets(data_path_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Full Categorical and Numerical Dataset.\n",
    "    \n",
    "<h4> Only a tiny subset of categorical columns actually have ANY data. I use a threshold of 0.001, which leaves me with only 31 categorical columns out of a few thousand. This is how sparse the data is. \n",
    "    \n",
    "<h4> In order to run ML at scale, I combine the numerical and categorical columns after loading them in chunks,\n",
    "     upsample the data to enhance the positive class (for training only) and subsequently saves the data into sharded\n",
    "    csvs that I can read into Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat_col = pd.read_csv(os.getcwd() + '/FInal_Numeric_Datsets/shuffled_train_categorical_test.csv'\n",
    "                           , dtype = str, chunksize = 10000).get_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_columns(data, threshold):\n",
    "    ''' extracts the dense(r) columns from the dataset'''\n",
    "    relevant_cols = []\n",
    "    for column in data.columns:\n",
    "        if len(data[column].value_counts())/data.shape[1] < threshold:\n",
    "            pass\n",
    "        else:\n",
    "            relevant_cols.append(column)\n",
    "    return relevant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = get_dense_columns(test_cat_col, 0.001)\n",
    "len(relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('categorical_cols.txt', 'w') as f:\n",
    "    json.dump(relevant_cols, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep these 31 columns.\n",
    "for col in relevant_cols:\n",
    "    print(test_cat_col[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sharded_full_datasets(full_data_path, full_cat_data_path, relevant_cols):\n",
    "    ''' Training dataset combining all numerical and 31 categorical columns. Upsampled'''\n",
    "    chunksize = 50000\n",
    "    for chunk1, chunk2 in zip(pd.read_csv(full_data_path, chunksize=chunksize)\n",
    "                              , pd.read_csv(full_cat_data_path, chunksize=chunksize, usecols = relevant_cols, dtype = str)):\n",
    "        print(\"Chunk Number: {}\".format(i))\n",
    "        chunk1 = chunk1.drop(columns = ['Id'])\n",
    "        chunk2 = chunk2.drop(columns = ['Id'])\n",
    "        full_chunk  = pd.concat([chunk1, chunk2], axis = 1)\n",
    "        temp_df = full_chunk[full_chunk['Response'] == 1]\n",
    "        print(len(temp_df))\n",
    "        pos_df = resample(temp_df, n_samples=25000,random_state=42,replace=True)\n",
    "        full_df = shuffle(pd.concat([full_chunk, pos_df]), random_state = 42)\n",
    "        print(len(full_df))\n",
    "        full_df.to_csv(os.path.dirname(full_data_path) + \"/entire_data_50k_{}.csv\".format(i),\n",
    "                    index=False, header = False) # make sure additional index doens't get added and drop Id column\n",
    "    return print(\"Done\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_sharded_full_datasets(data_path, data_path_cat, relevant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sharded_full_test_datasets(full_data_path, full_cat_data_path, relevant_cols):\n",
    "    ''' Test set combining numerical and 31 categorical columns. Not upsampled'''\n",
    "    chunksize = 50000\n",
    "    for chunk1, chunk2 in zip(pd.read_csv(full_data_path, chunksize=chunksize)\n",
    "                              , pd.read_csv(full_cat_data_path, chunksize=chunksize, usecols = relevant_cols, dtype = str)):\n",
    "        print(\"Chunk Number: {}\".format(i))\n",
    "        chunk1 = chunk1.drop(columns = ['Id'])\n",
    "        chunk2 = chunk2.drop(columns = ['Id'])\n",
    "        full_chunk  = pd.concat([chunk1, chunk2], axis = 1)\n",
    "        full_chunk.to_csv(os.path.dirname(full_data_path) + \"/test_entire_data_50k_{}.csv\".format(i),\n",
    "                        index=False, header = False) # make sure additional index doens't get added and drop Id column\n",
    "    return print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sharded_full_test_datasets(data_path_test, data_path_cat, relevant_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
